nemamo mysqlpump, niti nam odgovora mysqldump --tab (koji kreira hrpu fileova) / mysql import
nego zelimo handleati klasicni mysqldump (compressed) file.

research sto treba i bilo bi dobro

TODO:
- fix README.md (i u tablicu stavi usporedbu brzina)
- headere (Sve prije prve tablice / db) repliciraj za sve. a footere? check da li treba ili samo u master threadu. paziti fork i novi connection za svaki thread.
  vidi sto od headera treba samo jednom napraviti (tipa flush logs; stop slave, create/drop database, set master to...)
- stop slave i master za replikaciju;
	set sql_log_bin = 0;
	set session wsrep_on = 0;
- firewall clients;
- za import baza iz mysqldumpa paralelno X tablica
- i check da li je rezultat isti na kraju.
- set session bulk_insert_buffer_size = 512M ?
- SET GLOBAL read_only = ON; SET LOCAL read_only = OFF; ?
- set autocommit =0 na pocetku i commit na kraju? benchmark brzina.
- idi do DROP TABLE IF EXISTS...CREATE TABLE...LOCK TABLES...INSERT *x... UNLOCK TABLES
- vidi za brzinu:
  https://www.percona.com/blog/2018/02/22/restore-mysql-logical-backup-maximum-speed/
- posebno innodb_flush_log_at_trx_commit = 2 , key_cache_block_size=4096, myisam_block_size=4K, myisam_repair_threads = 2+
  bulk_insert_buffer_size=512M, SET SQL_LOG_BIN=0;
  i ostalo sa https://dba.stackexchange.com/q/13446/53319

- detect TAIL on procedures/functions too. create example db and dump to detect

- restart ovako? service mysql restart --innodb-doublewrite=0
- increase values na barem:
  max_allowed_packet=256M
  wait_timeout=30000
  https://dba.stackexchange.com/q/83125/53319

- mogucnost live importa da rsyncamo file i iz njega saljemo, a import da radi kao "tail -F" dok ne dodje do komandi koje su za kraj filea
  alternativno da moze direktno i bez snimanja na disk, ali da ima retry ako pukne i sl.
- publish github

- support za vise baza? koristi samo zadnji "^USE" onda

- zstd instead of lzop? ili gzip?
  https://github.com/facebook/zstd

- kako izbjeci serijalizaciju? npr. dok dodjem do documents.idb na njemu ce
  master blokirati i nece slati nista vise u paralelizaciju dok ne zavrsi.
  jedna ideja je koristiti uncompressed .sql file pa master samo da seeka i
  sprema pozicije gdje pocinju tablice, ali to nece raditi na pipeu (streamu
  od pigz -dc). mozda uvijek uncompress (sa onom "tail -F" detekcijom)? to bi radilo, 
  ali paziti koliko diska imamo slobodno!

  zapravo master write svaku tablicu u svoj file (compressed? LZO? uncompressed) 
  sa dodanim headerima, a onda svaki thread (parallel? xargs -P?) da cita
  svoju tablicu, importa, i rm-a kada je gotov. 
  ramdisk maybe possible ako nisu prevelike? ili mysql_tmp dir da koristi.
  (pa onda se mjesto smanjuje kako baza raste)

- ici po velicini ? nema smisla fork mysqla 100 puta za 100 malih tablica od 1K, 
  to moze sve ici u jedan mysql. ili pool threadova pa svaki uzima novo cim zavrsi, 
  a ne zatvara forkove mysqla

- mv/rm na starom strganom masteru; pa rsync -z raw baza sa slavea - brzina?
  za compare i za pisanje procedure...

- u global master threadu (pocetak i kraj) radi SET MASTER i postavljanje i gasenje globalnih varijabli
  (sql_bin_log=0, start/stop slave itd)

- skip CHANGE MASTER TO (ili prebaci u ".tail" file iz ".head")

- opcije za skip track_log, del_log itd (samo kreirati prazne tablice, no data)

- za TAIL da pocne odmah nakon views sto zavrse, iako nema posebnog separatora?

- zalba ako ne nadje terminatore za kraj tablica/baze itd gdje misli da bi trebali biti

- kada imam radecu scriptu, prijavi odgovor na onih par pitanja na stackoverflow i dba.stackexchange

- mozda cak split posebno CREATE TABLE, a posebno LOCK/INSERT/UNLOCK ?
  jer onda bi mogli create sve sa zadnjom vrijednosti autoinc u CREATE TABLE,
  pa onda u tablice track_log, del_log i sl. nebitne insertati bez lockanja as-we-go
  (scripta da sama pusti korisnike da se spajaju. I da ih disablea prije toga na startu
   staviti kao opciju ili kao default sa --disable-connections)

- optimal index creation?
  https://github.com/maxbube/mydumper/issues/4
  ne radi li disable keys/enable keys tocno to?
  https://dba.stackexchange.com/questions/76565/mysqldumps-disable-keys-have-no-effect-on-import
  
  mozemo li iz percone koristiti mysqldump? 
  https://www.percona.com/doc/percona-server/5.5/management/innodb_expanded_fast_index_creation.html
  https://www.percona.com/blog/2010/12/09/mysql-optimize-tables-innodb-stop/

- bulk loading
  https://dev.mysql.com/doc/refman/5.5/en/optimizing-myisam-bulk-data-loading.html
  https://dev.mysql.com/doc/refman/5.5/en/optimizing-innodb-bulk-data-loading.html

-  --skip-innodb-checksums utjece li na brzinu?

- pt-table-checksum na master/slave konfiguraciji probaj u tomsoftu

- unlink sve temp files prvo!
- views, functions, procedures da idu u .tail
- opcija da prvo pokrene najvece db? (sort by size) ili default po redoslijedu

- opcija da preskoci neku bazu u potpunisti bez zapisivanja na disk (ako je ne zelimo) 
  ili da je zapise ali ne importa (npr. track_log, del_log koje mozemo import i nakon pustanja klijenata u rad)


SADA;
- sto sam dodao u zz* da sa 75 se uspio na 85 min? nadji i izoliraj.

- usporedba brzina ramdisk i mysqlimport i ostali iz ticketa stavi u speedtest/
- probaj head + tableXXXX (od documents i track_log - probati uvijek i myisam i innodb) pa import sa:
	+ autocommit=0 + commit
	- sa paralelnih par INSERTova (Test del_log innodb)
	+ bez gasenja/paljenja indexa
	+ insert DELAYED ?
	+ sa razlicitim onim repair_threads koliko treba za track_log da se importa (sa insert, i sa load data infile)
	  i za neku innodb tablicu (del_log, documents?)

- probaj mydumper
	- upstream https://launchpad.net/mydumper (https://github.com/maxbube/mydumper) 0.9.5-1xenial
	- branch https://www.percona.com/blog/2015/01/21/importing-big-tables-large-indexes-myloader/ (bzr branch lp:~david-ducos/mydumper/mydumper)
	  for faster indexes
	- mydumper --rows 
	  https://github.com/maxbube/mydumper/issues/78


- provjeri sve backup/restore da li su uspjesni !!
	ispravno stanje restore regularni iz mysqldumpa je:
	mysql> select concat("select '",table_name,"', count(*) from ",table_name,";")  from `information_schema`.`tables`  WHERE `table_schema` = 'dupin' into outfile "/mysql/mysql-files/zbroji.sql";
	shell% mysql dupin < /mysql/mysql-files/zbroji.sql > dupin-p1.txt; diff dupin.txt dupin-p1.txt

- provjeri i mydumper i mysqlimport da li dobro ucitavaju sve tablice, i ako ne, zasto ne?
- vidi razlika velicina dupin i dupin2 zasto?? koje tablice/indexi? optimize table da li mijenja situaciju?
- vidi T:265096 za extra benchmark info i dodatne TODO (INSERT DELAYED za track_log npr. nakon enable klijenata)

- debug ispisi