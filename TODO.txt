nemamo mysqlpump, niti nam odgovora mysqldump --tab (koji kreira hrpu fileova) / mysql import
nego zelimo handleati klasicni mysqldump (compressed) file.

research sto treba i bilo bi dobro

TODO:
- fix README.md (i u tablicu stavi usporedbu brzina)
- headere (Sve prije prve tablice / db) repliciraj za sve. a footere? check da li treba ili samo u master threadu. paziti fork i novi connection za svaki thread.
  vidi sto od headera treba samo jednom napraviti (tipa flush logs; stop slave...)
- stop slave i master za replikaciju;
- firewall clients;
- za import baza iz mysqldumpa paralelno X tablica
- i check da li je rezultat isti na kraju.
- set session bulk_insert_buffer_size = 512M ?
- SET GLOBAL read_only = ON; SET LOCAL read_only = OFF; ?
- set autocommit =0 na pocetku i commit na kraju? benchmark brzina.
- idi do DROP TABLE IF EXISTS...CREATE TABLE...LOCK TABLES...INSERT *x... UNLOCK TABLES
- vidi za brzinu:
  https://www.percona.com/blog/2018/02/22/restore-mysql-logical-backup-maximum-speed/
- posebno innodb_flush_log_at_trx_commit = 2 , key_cache_block_size=4096, myisam_block_size=4K, myisam_repair_threads = 2+
  bulk_insert_buffer_size=512M, SET SQL_LOG_BIN=0;
  i ostalo sa https://dba.stackexchange.com/q/13446/53319

- restart ovako? service mysql restart --innodb-doublewrite=0
- increase values na barem:
  max_allowed_packet=256M
  wait_timeout=30000
  https://dba.stackexchange.com/q/83125/53319

- mogucnost live importa da rsyncamo file i iz njega saljemo, a import da radi kao "tail -F" dok ne dodje do komandi koje su za kraj filea
  alternativno da moze direktno i bez snimanja na disk, ali da ima retry ako pukne i sl.
- publish github

- support za vise baza? koristi samo zadnji "^USE" onda


- kako izbjeci serijalizaciju? npr. dok dodjem do documents.idb na njemu ce
  master blokirati i nece slati nista vise u paralelizaciju dok ne zavrsi.
  jedna ideja je koristiti uncompressed .sql file pa master samo da seeka i
  sprema pozicije gdje pocinju tablice, ali to nece raditi na pipeu (streamu
  od pigz -dc). mozda uvijek uncompress (sa onom "tail -F" detekcijom)? to bi radilo, 
  ali paziti koliko diska imamo slobodno!

  zapravo master write svaku tablicu u svoj file (compressed? LZO? uncompressed) 
  sa dodanim headerima, a onda svaki thread (parallel? xargs -P?) da cita
  svoju tablicu, importa, i rm-a kada je gotov. 
  ramdisk maybe possible ako nisu prevelike? ili mysql_tmp dir da koristi.
  (pa onda se mjesto smanjuje kako baza raste)

- ici po velicini ? nema smisla fork mysqla 100 puta za 100 malih tablica od 1K, 
  to moze sve ici u jedan mysql.

- mv/rm na starom strganom masteru; pa rsync -z raw baza sa slavea - brzina?
  za compare i za pisanje procedure...

- u global master threadu (pocetak i kraj) radi SET MASTER i postavljanje i gasenje globalnih varijabli
  (sql_bin_log=0, start/stop slave itd)

- skip CHANGE MASTER TO (ili prebaci u ".tail" file iz ".head")

- opcije za skip track_log, del_log itd (samo kreirati prazne tablice, no data)

SADA;
- probaj brzinu bez tomsoft-common i zz* konfiga 
- usporedba brzina ramdisk i mysqlimport i ostali iz ticketa stavi u speedtest/

- probaj head + tableXXXX (od documents i/ili track_log) pa import sa:
	- autocommit=0 + commit
	- sa paralelnih par INSERTova
	- bez gasenja/paljenja indexa
	- insert DELAYED ?
